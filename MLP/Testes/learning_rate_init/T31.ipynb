{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/naftalima/redes-neurais/blob/master/mlp_spine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m-xHBChIrJS2"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed May  8 14:54:58 2019\n",
    "\n",
    "@author: Delgado\n",
    "Fonte: https://www.kaggle.com/ahmethamzaemra/mlpclassifier-example/notebook\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "SWlPhr-brkni",
    "outputId": "548cbc7c-6857-4869-d588-3c37eb2231b3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>Col11</th>\n",
       "      <th>Col12</th>\n",
       "      <th>Class_att</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.027818</td>\n",
       "      <td>22.552586</td>\n",
       "      <td>39.609117</td>\n",
       "      <td>40.475232</td>\n",
       "      <td>98.672917</td>\n",
       "      <td>-0.254400</td>\n",
       "      <td>0.744503</td>\n",
       "      <td>12.5661</td>\n",
       "      <td>14.5386</td>\n",
       "      <td>15.30468</td>\n",
       "      <td>-28.658501</td>\n",
       "      <td>43.5123</td>\n",
       "      <td>Abnormal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.056951</td>\n",
       "      <td>10.060991</td>\n",
       "      <td>25.015378</td>\n",
       "      <td>28.995960</td>\n",
       "      <td>114.405425</td>\n",
       "      <td>4.564259</td>\n",
       "      <td>0.415186</td>\n",
       "      <td>12.8874</td>\n",
       "      <td>17.5323</td>\n",
       "      <td>16.78486</td>\n",
       "      <td>-25.530607</td>\n",
       "      <td>16.1102</td>\n",
       "      <td>Abnormal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.832021</td>\n",
       "      <td>22.218482</td>\n",
       "      <td>50.092194</td>\n",
       "      <td>46.613539</td>\n",
       "      <td>105.985135</td>\n",
       "      <td>-3.530317</td>\n",
       "      <td>0.474889</td>\n",
       "      <td>26.8343</td>\n",
       "      <td>17.4861</td>\n",
       "      <td>16.65897</td>\n",
       "      <td>-29.031888</td>\n",
       "      <td>19.2221</td>\n",
       "      <td>Abnormal</td>\n",
       "      <td>Prediction is done by using binary classificat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.297008</td>\n",
       "      <td>24.652878</td>\n",
       "      <td>44.311238</td>\n",
       "      <td>44.644130</td>\n",
       "      <td>101.868495</td>\n",
       "      <td>11.211523</td>\n",
       "      <td>0.369345</td>\n",
       "      <td>23.5603</td>\n",
       "      <td>12.7074</td>\n",
       "      <td>11.42447</td>\n",
       "      <td>-30.470246</td>\n",
       "      <td>18.8329</td>\n",
       "      <td>Abnormal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.712859</td>\n",
       "      <td>9.652075</td>\n",
       "      <td>28.317406</td>\n",
       "      <td>40.060784</td>\n",
       "      <td>108.168725</td>\n",
       "      <td>7.918501</td>\n",
       "      <td>0.543360</td>\n",
       "      <td>35.4940</td>\n",
       "      <td>15.9546</td>\n",
       "      <td>8.87237</td>\n",
       "      <td>-16.378376</td>\n",
       "      <td>24.9171</td>\n",
       "      <td>Abnormal</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Col1       Col2       Col3       Col4        Col5       Col6  \\\n",
       "0  63.027818  22.552586  39.609117  40.475232   98.672917  -0.254400   \n",
       "1  39.056951  10.060991  25.015378  28.995960  114.405425   4.564259   \n",
       "2  68.832021  22.218482  50.092194  46.613539  105.985135  -3.530317   \n",
       "3  69.297008  24.652878  44.311238  44.644130  101.868495  11.211523   \n",
       "4  49.712859   9.652075  28.317406  40.060784  108.168725   7.918501   \n",
       "\n",
       "       Col7     Col8     Col9     Col10      Col11    Col12 Class_att  \\\n",
       "0  0.744503  12.5661  14.5386  15.30468 -28.658501  43.5123  Abnormal   \n",
       "1  0.415186  12.8874  17.5323  16.78486 -25.530607  16.1102  Abnormal   \n",
       "2  0.474889  26.8343  17.4861  16.65897 -29.031888  19.2221  Abnormal   \n",
       "3  0.369345  23.5603  12.7074  11.42447 -30.470246  18.8329  Abnormal   \n",
       "4  0.543360  35.4940  15.9546   8.87237 -16.378376  24.9171  Abnormal   \n",
       "\n",
       "                                         Unnamed: 13  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2  Prediction is done by using binary classificat...  \n",
       "3                                                NaN  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Dataset_spine.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dWtxlUoWRuvO"
   },
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 13'], axis=1) #Removendo Coluna \"Unnamed: 13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "id": "sr6N2IFasxjG",
    "outputId": "9ca9a16e-3e93-4c74-bfed-482c35155c37",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Col7</th>\n",
       "      <th>Col8</th>\n",
       "      <th>Col9</th>\n",
       "      <th>Col10</th>\n",
       "      <th>Col11</th>\n",
       "      <th>Col12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>310.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>60.496653</td>\n",
       "      <td>17.542822</td>\n",
       "      <td>51.930930</td>\n",
       "      <td>42.953831</td>\n",
       "      <td>117.920655</td>\n",
       "      <td>26.296694</td>\n",
       "      <td>0.472979</td>\n",
       "      <td>21.321526</td>\n",
       "      <td>13.064511</td>\n",
       "      <td>11.933317</td>\n",
       "      <td>-14.053139</td>\n",
       "      <td>25.645981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.236520</td>\n",
       "      <td>10.008330</td>\n",
       "      <td>18.554064</td>\n",
       "      <td>13.423102</td>\n",
       "      <td>13.317377</td>\n",
       "      <td>37.559027</td>\n",
       "      <td>0.285787</td>\n",
       "      <td>8.639423</td>\n",
       "      <td>3.399713</td>\n",
       "      <td>2.893265</td>\n",
       "      <td>12.225582</td>\n",
       "      <td>10.450558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>26.147921</td>\n",
       "      <td>-6.554948</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>13.366931</td>\n",
       "      <td>70.082575</td>\n",
       "      <td>-11.058179</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>7.027000</td>\n",
       "      <td>7.037800</td>\n",
       "      <td>7.030600</td>\n",
       "      <td>-35.287375</td>\n",
       "      <td>7.007900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>46.430294</td>\n",
       "      <td>10.667069</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>33.347122</td>\n",
       "      <td>110.709196</td>\n",
       "      <td>1.603727</td>\n",
       "      <td>0.224367</td>\n",
       "      <td>13.054400</td>\n",
       "      <td>10.417800</td>\n",
       "      <td>9.541140</td>\n",
       "      <td>-24.289522</td>\n",
       "      <td>17.189075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>58.691038</td>\n",
       "      <td>16.357689</td>\n",
       "      <td>49.562398</td>\n",
       "      <td>42.404912</td>\n",
       "      <td>118.268178</td>\n",
       "      <td>11.767934</td>\n",
       "      <td>0.475989</td>\n",
       "      <td>21.907150</td>\n",
       "      <td>12.938450</td>\n",
       "      <td>11.953835</td>\n",
       "      <td>-14.622856</td>\n",
       "      <td>24.931950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>72.877696</td>\n",
       "      <td>22.120395</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>52.695888</td>\n",
       "      <td>125.467674</td>\n",
       "      <td>41.287352</td>\n",
       "      <td>0.704846</td>\n",
       "      <td>28.954075</td>\n",
       "      <td>15.889525</td>\n",
       "      <td>14.371810</td>\n",
       "      <td>-3.497094</td>\n",
       "      <td>33.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>129.834041</td>\n",
       "      <td>49.431864</td>\n",
       "      <td>125.742385</td>\n",
       "      <td>121.429566</td>\n",
       "      <td>163.071041</td>\n",
       "      <td>418.543082</td>\n",
       "      <td>0.998827</td>\n",
       "      <td>36.743900</td>\n",
       "      <td>19.324000</td>\n",
       "      <td>16.821080</td>\n",
       "      <td>6.972071</td>\n",
       "      <td>44.341200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Col1        Col2        Col3        Col4        Col5        Col6  \\\n",
       "count  310.000000  310.000000  310.000000  310.000000  310.000000  310.000000   \n",
       "mean    60.496653   17.542822   51.930930   42.953831  117.920655   26.296694   \n",
       "std     17.236520   10.008330   18.554064   13.423102   13.317377   37.559027   \n",
       "min     26.147921   -6.554948   14.000000   13.366931   70.082575  -11.058179   \n",
       "25%     46.430294   10.667069   37.000000   33.347122  110.709196    1.603727   \n",
       "50%     58.691038   16.357689   49.562398   42.404912  118.268178   11.767934   \n",
       "75%     72.877696   22.120395   63.000000   52.695888  125.467674   41.287352   \n",
       "max    129.834041   49.431864  125.742385  121.429566  163.071041  418.543082   \n",
       "\n",
       "             Col7        Col8        Col9       Col10       Col11       Col12  \n",
       "count  310.000000  310.000000  310.000000  310.000000  310.000000  310.000000  \n",
       "mean     0.472979   21.321526   13.064511   11.933317  -14.053139   25.645981  \n",
       "std      0.285787    8.639423    3.399713    2.893265   12.225582   10.450558  \n",
       "min      0.003220    7.027000    7.037800    7.030600  -35.287375    7.007900  \n",
       "25%      0.224367   13.054400   10.417800    9.541140  -24.289522   17.189075  \n",
       "50%      0.475989   21.907150   12.938450   11.953835  -14.622856   24.931950  \n",
       "75%      0.704846   28.954075   15.889525   14.371810   -3.497094   33.979600  \n",
       "max      0.998827   36.743900   19.324000   16.821080    6.972071   44.341200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "Q_askBp9nfc6",
    "outputId": "57295e48-8d81-4451-b450-72075c7e26f8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 310 entries, 0 to 309\n",
      "Data columns (total 13 columns):\n",
      "Col1         310 non-null float64\n",
      "Col2         310 non-null float64\n",
      "Col3         310 non-null float64\n",
      "Col4         310 non-null float64\n",
      "Col5         310 non-null float64\n",
      "Col6         310 non-null float64\n",
      "Col7         310 non-null float64\n",
      "Col8         310 non-null float64\n",
      "Col9         310 non-null float64\n",
      "Col10        310 non-null float64\n",
      "Col11        310 non-null float64\n",
      "Col12        310 non-null float64\n",
      "Class_att    310 non-null object\n",
      "dtypes: float64(12), object(1)\n",
      "memory usage: 31.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "mKnxdolXtEKB",
    "outputId": "9753e3c6-3857-4aa6-f7d8-565c75c59250"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCol7 -> pelvic_slope -> inclinação pélvica\\nCol8 -> Direct_tilt -> Inclinação direta\\nCol9 -> thoracic_slope -> inclinação torácico\\nCol10 -> cervical_tilt - >inclinação cervical\\nCol11 -> sacrum_angle -> ângulo de sacro\\nCol12 -> scoliosis_slope -> inclinação da escoliose\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Col7 -> pelvic_slope -> inclinação pélvica\n",
    "Col8 -> Direct_tilt -> Inclinação direta\n",
    "Col9 -> thoracic_slope -> inclinação torácico\n",
    "Col10 -> cervical_tilt - >inclinação cervical\n",
    "Col11 -> sacrum_angle -> ângulo de sacro\n",
    "Col12 -> scoliosis_slope -> inclinação da escoliose\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "Sh4eauwAX9X2",
    "outputId": "ea2d5a18-b480-42df-8f4e-35bfa67451f3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3', 'Col4', 'Col5', 'Col6', 'Col7', 'Col8', 'Col9',\n",
    "       'Col10', 'Col11', 'Col12'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "BVDtyP0GtFOu",
    "outputId": "7cd858f5-325b-47aa-b0ea-b9b364d5ff92",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Col3</th>\n",
       "      <th>Col4</th>\n",
       "      <th>Col5</th>\n",
       "      <th>Col6</th>\n",
       "      <th>Class_att</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63.027818</td>\n",
       "      <td>22.552586</td>\n",
       "      <td>39.609117</td>\n",
       "      <td>40.475232</td>\n",
       "      <td>98.672917</td>\n",
       "      <td>-0.254400</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.056951</td>\n",
       "      <td>10.060991</td>\n",
       "      <td>25.015378</td>\n",
       "      <td>28.995960</td>\n",
       "      <td>114.405425</td>\n",
       "      <td>4.564259</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.832021</td>\n",
       "      <td>22.218482</td>\n",
       "      <td>50.092194</td>\n",
       "      <td>46.613539</td>\n",
       "      <td>105.985135</td>\n",
       "      <td>-3.530317</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.297008</td>\n",
       "      <td>24.652878</td>\n",
       "      <td>44.311238</td>\n",
       "      <td>44.644130</td>\n",
       "      <td>101.868495</td>\n",
       "      <td>11.211523</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49.712859</td>\n",
       "      <td>9.652075</td>\n",
       "      <td>28.317406</td>\n",
       "      <td>40.060784</td>\n",
       "      <td>108.168725</td>\n",
       "      <td>7.918501</td>\n",
       "      <td>Abnormal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Col1       Col2       Col3       Col4        Col5       Col6 Class_att\n",
       "0  63.027818  22.552586  39.609117  40.475232   98.672917  -0.254400  Abnormal\n",
       "1  39.056951  10.060991  25.015378  28.995960  114.405425   4.564259  Abnormal\n",
       "2  68.832021  22.218482  50.092194  46.613539  105.985135  -3.530317  Abnormal\n",
       "3  69.297008  24.652878  44.311238  44.644130  101.868495  11.211523  Abnormal\n",
       "4  49.712859   9.652075  28.317406  40.060784  108.168725   7.918501  Abnormal"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop(['Col7','Col8','Col9','Col10','Col11','Col12'], axis=1)\n",
    "\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OZDhfE0z1S9i"
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y = df['Class_att']\n",
    "x = df.drop(['Class_att'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.25, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2006
    },
    "colab_type": "code",
    "id": "0gItbF3OtSSG",
    "outputId": "3b721fa1-5bc1-486c-daa1-2b91842e555e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = inf\n",
      "Iteration 2, loss = inf\n",
      "Iteration 3, loss = inf\n",
      "Iteration 4, loss = inf\n",
      "Iteration 5, loss = inf\n",
      "Iteration 6, loss = inf\n",
      "Iteration 7, loss = inf\n",
      "Iteration 8, loss = 58912843902517994049928833069072329650715571271337638638299095590754348077027870064484980872457129311212598769215593021120844502798403826312675625144911487041536.00000000\n",
      "Iteration 9, loss = 78953642833428582048996288486423698212967137866777534102364740153882225845941527850980654732617827070478435995365126370258634788654655303607688881828205250478080.00000000\n",
      "Iteration 10, loss = 97350806146054768221641682650564119178982420363167832764740964096429694668221830153635422570175595544628564305367422782677680269153271990112365918424641715568640.00000000\n",
      "Iteration 11, loss = 113672299212273028523921679297146928739935683187857630209801603852796963790171403130453653196681437809814736063025494744239820951199877866444808979996132868358144.00000000\n",
      "Iteration 12, loss = 127824144593790314662467107074839984731723549934605495364153174683957237336443626173785497084494684941491281809067881301294759249388442672827376428732336121577472.00000000\n",
      "Iteration 13, loss = 139898163923339339149474672024028067995570804949662167473844576602115459254231872536962992524082683240458762218874930501356581579492038696955078159879867023228928.00000000\n",
      "Iteration 14, loss = 150078921034376752889575192782793117398862773477487152788438675988724516392787148716366974475385235897192931492682552835614131791494795848297716074424366983544832.00000000\n",
      "Iteration 15, loss = 158588207618040675070734646024899639260266432943652217067356792140195766326400390789070336192646734886102197799503038188175916124927179143479459597589510709313536.00000000\n",
      "Iteration 16, loss = 165653107964430174061820820665225221460898290076757232629550381534597089333228840556403182979479797778480930968031775925069987297997826268525803214843824863969280.00000000\n",
      "Iteration 17, loss = 171488681332246933496461610858887637066719407004280295496241103122106833200200324779489376598519421101777726892889377756676288932518484650644727790688655894380544.00000000\n",
      "Iteration 18, loss = 176289544312293002738304837851984389303790985291721323346872518871494814793799270586855856903941990829872879199804346402192514873305022876833055230981960462499840.00000000\n",
      "Iteration 19, loss = 180226733052541017819820140481017504602162789136817093824451634080371749144697823296277090884151518924645988797422557200071099830911038623934129028323983981805568.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 20, loss = 183447590400433742543188837599732824404392816354762591770389905596336902938949570205502507007234291577983961562164725093213907629654069875091005751452796438183936.00000000\n",
      "Iteration 21, loss = 186077450761388301293303301576015587377298830006089359959830019176447806612945046284652845149487593632662848270151554829525437717878999697662176594812619420860416.00000000\n",
      "Iteration 22, loss = 188221346787082579809657521042972367074186860278488555374934909639892356771214226338723827401820399079376492962943999020133135500018320184205124187803910779437056.00000000\n",
      "Iteration 23, loss = 189966878405515425898417825906026118585172260843221030534042551445040290283216719822328452261062037760234832975707966293555303069267134799833725890936882725912576.00000000\n",
      "Iteration 24, loss = 191386629339401859963506231740272183958575934431405755875682775535369411496483062285835755141992376280419986625976211185815828086114639963817454542800378174373888.00000000\n",
      "Iteration 25, loss = 192540460391784819356943954649739132285591372109146204540866786959777410502506080396288984178663815006361370785360670276037000115149909048598864429321141887172608.00000000\n",
      "Iteration 26, loss = 193477559520018238801174516159712553069693940901272259293272340010645825264425876121727795722983514323648506165618184252883068054035489679145037850306723677470720.00000000\n",
      "Iteration 27, loss = 194238228703366405860815890816490717894558809441459505868165616176405641854596154225928645016175851267025410150175889457513389897108594958797587093053633059618816.00000000\n",
      "Iteration 28, loss = 194855414176306799860476580777796177512614109350947764887177858093896697732272537328637566473858774793455835740285052520915193672366162201935747869847016506916864.00000000\n",
      "Iteration 29, loss = 195356000288920718225350331984036428214876750511337253099094857152189782186097100680146845351088309797135392263659422538269009165957738730402534018156390466453504.00000000\n",
      "Iteration 30, loss = 195761893206925617832923253950505883658979767021974335075342508655563278852398819988768334967533055040671924427557984180509745662947037730314028101684782010728448.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 31, loss = 196090925094430818488558871547570220318995391717078324712788239198736481765226375303240349153081700865521255446632878948497663489555924532522154562781636464738304.00000000\n",
      "Iteration 32, loss = 196357642415980059086818331461246792667846000413569560824306782312388951507506693364235622741762259706174657140567793842026810197067058857049549792329988155899904.00000000\n",
      "Iteration 33, loss = 196573805866753749512657249074021342248539749884905724840216610192513815677209514616190887405382052215841849104489132357000364785738144934258287443490465671806976.00000000\n",
      "Iteration 34, loss = 196748974839047992074673159700531328228826350074209962532526334084155647457623368201465208226735470745069581224884958255175942733190727384988114216138776374149120.00000000\n",
      "Iteration 35, loss = 196890908206167533069185681844073919675834055491533783979430386079532401221245472624788594869158337650731561753733647722560904244622447346556838786860507161690112.00000000\n",
      "Iteration 36, loss = 197005900999248354079406619583221636737162606313375883807464155332248290019990496894419044303041233107997927070497748466661908924158083284867844431144115148685312.00000000\n",
      "Iteration 37, loss = 197099058980038753135045858368844670352895362854130782411954705296261154726493611458653846565810459265208736364409763505610269765793282828296308736412286460100608.00000000\n",
      "Iteration 38, loss = 197174522268185508403552749662415624921633107657766913597143862062721355787256342248961653456885272061362621517177872903799495299056584082088858972101807907536896.00000000\n",
      "Iteration 39, loss = 197235647281965197679372326850828864408900104921949532381528048138177608939724381386707106797198960182957151857421029810160506622444418128571968329857754023854080.00000000\n",
      "Iteration 40, loss = 197285154636880571537792989194519235945697561776079169694173762618089265237345183529530510779698730696261901782379316828409576243162613950297179863313359577284608.00000000\n",
      "Iteration 41, loss = 197325249289029391880400631231469441050483561543598991389081000024499391989484611810698043179205716240193172187364669186408494845717944940565554040623009693171712.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 42, loss = 197357718677885804368556986800065958453101334561853865818572171860399413375611201758594966011393632949017692132367715606475972515878588950030877919175080534867968.00000000\n",
      "Iteration 43, loss = 197384020525688172485227578908283528519757223173150770047148234544163955876330171592404111340444896570536647896459464300566613113965967861399753860694569354526720.00000000\n",
      "Iteration 44, loss = 197405324135295319829622514887961670251300009675342971294421553904258004443293753204187007800822609004154673697873397370912699924212071481278165746804998595936256.00000000\n",
      "Iteration 45, loss = 197422578728566455095874487627413728174755789757969470100671568231328907917678756383815787195606330189996798316635630995975863490800119951586224792113675858083840.00000000\n",
      "Iteration 46, loss = 197436553327693841083807841987379574567329368060234020083096278789369161877226653719101706673722327743737948314466251267288254854808707067690711660055230649729024.00000000\n",
      "Iteration 47, loss = 197447870940699123331893751990169427347273864878496090956052563529082993667690622475577460792372518461942685879672557781986975567541062912529892707659662529396736.00000000\n",
      "Iteration 48, loss = 197457036269719905968182678546563507542177391147704119594094448141101831357015767259085054329799093725611385995087226917508176482038639130042806890143294081204224.00000000\n",
      "Iteration 49, loss = 197464458166553559140719324480664960545941971447154266059679233492897872366215494382843561340010679195767819760500741716558524912623699598118792021833175112089600.00000000\n",
      "Iteration 50, loss = 197470467829411508413386850473195114852793227928923168378903286789116166024050978211211287158293449290044713836423115088313382116009296708802140654057802960994304.00000000\n",
      "Iteration 51, loss = 197475333547383498523288515151232043613152821552949208129288768919023242111263417698663986434344914550420270409235416616430767006008479963249090246816209334960128.00000000\n",
      "Iteration 52, loss = 197479272646794818784826594431195396003470446550668522394022450143435089846533581444173322784208358856891784773346721969501816242041986978087685688535259117780992.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Setting learning rate to 0.000160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 53, loss = 197482461290018349003225494345229216826663454505765407746696965017567497186200416376034469765346192212114144971169781783919415034096880002378299544494839793975296.00000000\n",
      "Iteration 54, loss = 197485044045806528262102663235025494341774110186435313860386785736042484700157654955744561742607006791388440286110497259203648413050873674953594864261581500317696.00000000\n",
      "Iteration 55, loss = 197487135655149304828724535667086806693795483698019466547365494078562091303488593109694700559837819669251914219978332840315423412615921254097439724576266256384000.00000000\n",
      "Iteration 56, loss = 197488829431594452759993150847691708658564429386415162648827068710807002663502525069815682345354600813392318956594618856267956397052357157910108371646843354873856.00000000\n",
      "Iteration 57, loss = 197490200960586634211769331795279674603381777017081602992196001213088796174037329856976195361900228427530880485792208702858660035281559203198684987164384584269824.00000000\n",
      "Iteration 58, loss = 197491311467300865603445929000136681574212264527424981775078849465044349509275886971545597590413264912078066548258869804215904182630788685599295660353587320455168.00000000\n",
      "Iteration 59, loss = 197492210544762201132122222857980124339767647303100308830926214883122809740228883761306480139376530807907204739347776290010990434487249075987081770882957337165824.00000000\n",
      "Iteration 60, loss = 197492938363736151729149291933493939125989219903576542746816964271086076605835714882140202981099573208032982861236310080159960155787593266848382503849877705850880.00000000\n",
      "Iteration 61, loss = 197493527462815474475947668644982166151856329036243031662233713018730366591967496407707703809833120723132494390067122966957514370866441757391212278568869194366976.00000000\n",
      "Iteration 62, loss = 197494004198439083899245160999965409515453608251148847486348545423611858627594793547007854607078305410254190001307297726576911418165410299362025343068870740541440.00000000\n",
      "Iteration 63, loss = 197494389919439679718235490670050012211958240548099535685025327776660475235750489153037036822492878483989858957032057442549681629669330513433899620702831090073600.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 64, loss = 197494701942464324890765527018845735496477006802976697375296082688632998081102034118430595269808577744189174605029286537957830841951580990188542399006121626959872.00000000\n",
      "Iteration 65, loss = 197494954668466116402638536755820136530679207536253787812098311450584487198579613562126094210361259890097531324479321745154389029528200668114976036664011198038016.00000000\n",
      "Iteration 66, loss = 197495159289590527092337189139452069666374399038393674470609062236944942440266076760828959752515191579721933322388008006126465084688831296508000228512596725071872.00000000\n",
      "Iteration 67, loss = 197495324945723330944921013181208784469846319201964321808062715372509668076228036247324213164455111700124775410392077054514318564353582987095264810747715091693568.00000000\n",
      "Iteration 68, loss = 197495459040186016294741581654630707740263220984139198582834804963678684736584473921650501203838757122115099318201277668978050541401419027434367013884700101967872.00000000\n",
      "Iteration 69, loss = 197495567569678210088101417922057707912640419910042691388235094548217955012657941700925862226429480485652190157083954796200721030315803002131279157892704093863936.00000000\n",
      "Iteration 70, loss = 197495655391532768986785513321738526544883035896966838659148193460002015617002212341411990039592220378607680769207529642351479443526518386119096976219055534899200.00000000\n",
      "Iteration 71, loss = 197495726440193207823828185701546978742792565618533854976069709407419354889322589640205685806411479391663981896294774704160466114817659945884036851597409223966720.00000000\n",
      "Iteration 72, loss = 197495783902561341689214664466110078499213338430707495175977166546132191161661635888762581160975440391886470390176290624005712428498603153602646863757049914720256.00000000\n",
      "Iteration 73, loss = 197495830360029602512622004002693907344048463137524933095895210110098121177207255126762412067611215101953266599189540757456940497781769641245847822037184730365952.00000000\n",
      "Iteration 74, loss = 197495867903526663570295527688744659684382517439536727003031798995798183410576778812016467312303758179212332425781568115584618639577689303201106403635677867540480.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 75, loss = 197495898231508851765595396227640618267132816472620409416395095670244519424644937669307015178897705203907414713922500668090976583400297453423434652004392267415552.00000000\n",
      "Iteration 76, loss = 197495922794610262650078902048063226054858555456391356297656950330035683822671073672531190524865666106641568970776139660830420858692890297608860428296207170797568.00000000\n",
      "Iteration 77, loss = 197495942673312273447583430881179647695748464045328784972864014279022065993544640891031562355026486692108745461070209104266462767350211761694677999138089560178688.00000000\n",
      "Iteration 78, loss = 197495958757650444763566103415547859340006116374795352667296306355663809001903156045155598765015282279881486622047953512407448663606592786402828114554666285531136.00000000\n",
      "Iteration 79, loss = 197495971768553520001655369266010843611046260315281827980682748412074456028091712561158146256052841140062908273592481594564598094247092346209435576701618741051392.00000000\n",
      "Iteration 80, loss = 197495982289974134700726168390912008830641254228311544220113516823200254304757665706386464891596199390087401017100246697688601835397319975151008370758945528086528.00000000\n",
      "Iteration 81, loss = 197495990794913785291371452151733820907037483953587730586461971541226258012508922650916370831033272553110333465738214229279932114977492669542835858665640738947072.00000000\n",
      "Iteration 82, loss = 197495997666503772290290456100177069952748069763739781532734934399405353368962984793036918539230634009956392954264249415712660795838546595486973661036529462542336.00000000\n",
      "Iteration 83, loss = 197496003215080540520807114285371930812075527423506938884821548313027675366505806096246754775467565321656273922877138650082329321675658426242814720130613789065216.00000000\n",
      "Iteration 84, loss = 197496007692016523630074056515169501955765795644031340940607578663879085498772810070660313778066670012947417825600779514000718331634069689514662462134885308956672.00000000\n",
      "Iteration 85, loss = 197496011300923425338469836220248379513734274630267699506801179126906100510841069863336371005631706190050624100511029110773453806050304135401949726858974395891712.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 86, loss = 197496014207687417303433121609051790782736776832773189477128849251536945234896705252861972257611283502026775968161852996798329767664857962601684323987033056870400.00000000\n",
      "Iteration 87, loss = 197496016561653100016011396198502453552745408390543421929393656813488929858951562242218168149310611694126276376453987228616132883313861309113850428409678013661184.00000000\n",
      "Iteration 88, loss = 197496018464883070424166125169017413971284978575705740472595016537566573260531606478221839213692744274599878878101429164002825109458989273299931845768454982336512.00000000\n",
      "Iteration 89, loss = 197496020003017155572298414409850666743273599085757493550726169516980575750244032978976395800016633216946041003619405034568117297098460060986839741896990572675072.00000000\n",
      "Iteration 90, loss = 197496021245423505231084304543092024436944673292764266592879167817049402268623134126030654729850951775299197272728505569221745272315174667489790195829816732680192.00000000\n",
      "Iteration 91, loss = 197496022248290396635696132157565302433011886497903739602525403222779398312198944837479845725366206611352666150527147865371559926845351716308876751876002989211648.00000000\n",
      "Iteration 92, loss = 197496023057130276406970396025401976747408738833963198688495175218161143787207622750081632366670598688196762735334065045530078869688407352964746573335320374280192.00000000\n",
      "Iteration 93, loss = 197496023708808397629345101493682201175097336457467205082007095838843767568234399790010748582389002988333578381226808190234421306929333157514029141821681033019392.00000000\n",
      "Iteration 94, loss = 197496024233185359068094022956898548488496363932110277535412690892871001524090067766956406528345468772864867633587200876125147350441365051432750581459895862689792.00000000\n",
      "Iteration 95, loss = 197496024654448518439042273279671336129598950485493446435650439496638007524463360919241971345482049653224215300833120722761813456924400277451647395415666047582208.00000000\n",
      "Iteration 96, loss = 197496024992189395242239761563580894098696725909894655984027450238245096563082541211300349228799684595915133311440948141707041252851657568506407873226772374880256.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 97, loss = 197496025262469335821085462783209838037519854157729551646984717049324217450708792233411413177567426576301638841156454136349569870514250432299359374181512899461120.00000000\n",
      "Iteration 98, loss = 197496025481293520019061066580913765890440060533240750113180405700561061046085717887530190968017124022341430718180302103933402094244157476554407502738954903355392.00000000\n",
      "Iteration 99, loss = 197496025657844583683915265369270571435282862897452975813656099492092440982579187644881978450795137817420182829647007601659879990914830503524081467280692164427776.00000000\n",
      "Iteration 100, loss = 197496025800154536095727081880246593816527694701752911881637355997386899384748512959710926985221405940232014937993112537149893088706425518793135682836984802312192.00000000\n",
      "Iteration 101, loss = 197496025914729176155320251507854634819397029563311974812793399054142539976219999544639159798001529196907717834923004070706799703492005594428623238904988894756864.00000000\n",
      "Iteration 102, loss = 197496026006838165259558584306293936884448762278216474204653547883624860944375336912637904604648350064111413015369085733756228042236985894880015366919416960778240.00000000\n",
      "Iteration 103, loss = 197496026080749932636260142773377185535923684999642583269646470839826604783249488349630203332864765482683874321337612085759099808558770346253116525145265683824640.00000000\n",
      "Iteration 104, loss = 197496026139922052057789712320769084707207193691846111344232466010131753121556484584611096709121143605031751936277899599779138064217407635658890115468934711869440.00000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 105, loss = 197496026187155031661641568315273139746757818675048902827803708354091452231148408102988841192164930554988892253549544376417632204985912560287322941485555736641536.00000000\n",
      "Iteration 106, loss = 197496026224717333736382530935714027623965981349651457041101696468073620544934470033981162946575228446631824602883430892750988053083982927937429345748938802593792.00000000\n",
      "Iteration 107, loss = 197496026254446319582774983097731552257176586021895614895758467092397130356327456171161733783855005602428422326761156165975469661735946762758816675765236506034176.00000000\n",
      "Training loss did not improve more than tol=0.000000 for 10 consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(200, 200, 200, 200), learning_rate='adaptive',\n",
       "              learning_rate_init=0.1, max_iter=500, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=21, shuffle=True, solver='sgd', tol=1e-09,\n",
       "              validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(200,200,200,200), max_iter=500, alpha=0.0001,\n",
    "                     solver='sgd', verbose=10,  random_state=21, tol=0.000000001, learning_rate='adaptive',\n",
    "                    learning_rate_init=0.1)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "t8cJt64ZthJ7",
    "outputId": "fd8d77f6-9b3a-42d9-8eed-9fcc687eae94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6794871794871795\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "nBofa_EGtuoM",
    "outputId": "5e780c82-0c10-4d96-d9c5-82bfa53fbab4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "sns.heatmap(cm, center=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "LBv_xhBUwVQT",
    "outputId": "a34df316-436a-46b8-9016-b85846f0e651"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Abnormal       0.68      1.00      0.81        53\n",
      "      Normal       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.68        78\n",
      "   macro avg       0.34      0.50      0.40        78\n",
      "weighted avg       0.46      0.68      0.55        78\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "mlp_spine.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
